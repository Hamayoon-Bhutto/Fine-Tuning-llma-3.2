{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Parameter-Efficient Supervised Fine-Tuning of LLaM","metadata":{}},{"cell_type":"markdown","source":"## Enviroment pakages ","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install unsloth vllm\n!pip install --force-reinstall --no-cache=dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:05:54.110710Z","iopub.execute_input":"2025-06-26T03:05:54.110922Z","iopub.status.idle":"2025-06-26T03:05:59.125896Z","shell.execute_reply.started":"2025-06-26T03:05:54.110904Z","shell.execute_reply":"2025-06-26T03:05:59.125058Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:05:59.127073Z","iopub.execute_input":"2025-06-26T03:05:59.127744Z","iopub.status.idle":"2025-06-26T03:06:02.273043Z","shell.execute_reply.started":"2025-06-26T03:05:59.127717Z","shell.execute_reply":"2025-06-26T03:06:02.272241Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### import libraries","metadata":{}},{"cell_type":"code","source":"import unsloth\nfrom unsloth import FastLanguageModel\nfrom unsloth import is_bfloat16_supported\nimport torch\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:06:02.274215Z","iopub.execute_input":"2025-06-26T03:06:02.274896Z","iopub.status.idle":"2025-06-26T03:06:18.642791Z","shell.execute_reply.started":"2025-06-26T03:06:02.274871Z","shell.execute_reply":"2025-06-26T03:06:18.641922Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-06-26 03:06:07.987705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750907168.010954     416 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750907168.018058     416 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\nINFO 06-26 03:06:14 [importing.py:53] Triton module has been replaced with a placeholder.\nINFO 06-26 03:06:14 [__init__.py:239] Automatically detected platform cuda.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom datasets import load_dataset, DatasetDict\nfrom huggingface_hub import login\nimport wandb\nimport numpy as np\nfrom rouge_score import rouge_scorer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:06:18.643649Z","iopub.execute_input":"2025-06-26T03:06:18.643927Z","iopub.status.idle":"2025-06-26T03:06:19.037085Z","shell.execute_reply.started":"2025-06-26T03:06:18.643902Z","shell.execute_reply":"2025-06-26T03:06:19.036454Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Logging into Hugging Face and Weights & Biases ","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom trl import SFTTrainer\nfrom unsloth import is_bfloat16_supported\nfrom huggingface_hub import login \nfrom transformers import TrainingArguments\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:06:19.038975Z","iopub.execute_input":"2025-06-26T03:06:19.039226Z","iopub.status.idle":"2025-06-26T03:06:19.043081Z","shell.execute_reply.started":"2025-06-26T03:06:19.039208Z","shell.execute_reply":"2025-06-26T03:06:19.042341Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Initialize Hugging Face & WnB tokens\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient() # from kaggle_secrets import UserSecretsClient\nhugging_face_token = user_secrets.get_secret(\"hf_token\")\nwnb_token = user_secrets.get_secret(\"wnbs\")\n\n# Login to Hugging Face\n\nlogin(hugging_face_token)\n\n# Login to WnB\n\nwandb.login(key=wnb_token)  #import wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:06:19.043906Z","iopub.execute_input":"2025-06-26T03:06:19.044145Z","iopub.status.idle":"2025-06-26T03:06:25.011859Z","shell.execute_reply.started":"2025-06-26T03:06:19.044128Z","shell.execute_reply":"2025-06-26T03:06:25.011285Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhamayoonali38\u001b[0m (\u001b[33mhamayoonali38-datacamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Loading Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"max_seq_length = 2048\ndtype =None\nload_in_4bit =True\n\nmodel , tokenizer =FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:06:25.012622Z","iopub.execute_input":"2025-06-26T03:06:25.013216Z","iopub.status.idle":"2025-06-26T03:06:32.695321Z","shell.execute_reply.started":"2025-06-26T03:06:25.013194Z","shell.execute_reply":"2025-06-26T03:06:32.694679Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.6.5: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Dataset preparation","metadata":{}},{"cell_type":"code","source":"dataset=load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\",split=\"train[0:500]\",trust_remote_code =True)\ndataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:06:32.696443Z","iopub.execute_input":"2025-06-26T03:06:32.696709Z","iopub.status.idle":"2025-06-26T03:06:33.992069Z","shell.execute_reply.started":"2025-06-26T03:06:32.696690Z","shell.execute_reply":"2025-06-26T03:06:33.991412Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Question', 'Complex_CoT', 'Response'],\n    num_rows: 500\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from datasets import DatasetDict\n\nvalidation_size = 100\ntrain_size = len(dataset) - validation_size\nindices = np.random.permutation(len(dataset))\ntrain_indices, val_indices = indices[:train_size], indices[train_size:]\n\ntrain_dataset = dataset.select(train_indices)\nval_dataset = dataset.select(val_indices)\n\ndataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\n\n\ndef formatting_prompts_func(examples):\n    inputs = examples[\"Question\"]\n    cots = examples[\"Complex_CoT\"]\n    outputs = examples[\"Response\"]\n    texts = []\n\n    for input_text, cot, output in zip(inputs, cots, outputs):\n        convo = [\n            {\"role\": \"user\", \"content\": input_text},\n            {\"role\": \"assistant\", \"content\": f\"</think>{cot}</think>\\n<response>{output}</response>\"}\n        ]\n        text = tokenizer.apply_chat_template(\n            convo,\n            tokenize=False,\n            add_generation_prompt=False\n        )\n        texts.append(text)\n\n    return {\"text\": texts}\n\n\ndataset_dict = dataset_dict.map(\n    formatting_prompts_func,\n    batched=True,\n    num_proc=2\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:06:33.992876Z","iopub.execute_input":"2025-06-26T03:06:33.993132Z","iopub.status.idle":"2025-06-26T03:06:35.013816Z","shell.execute_reply.started":"2025-06-26T03:06:33.993115Z","shell.execute_reply":"2025-06-26T03:06:35.013012Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed4e982b148a40289933d4c170b802b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fce12e72d25b40dda12fbf2a658b8dd9"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Fine-Tunning Setup with LoRA","metadata":{}},{"cell_type":"code","source":"model_lora = FastLanguageModel.get_peft_model(\n\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:06:35.014792Z","iopub.execute_input":"2025-06-26T03:06:35.015026Z","iopub.status.idle":"2025-06-26T03:06:41.764096Z","shell.execute_reply.started":"2025-06-26T03:06:35.014996Z","shell.execute_reply":"2025-06-26T03:06:41.763480Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.6.5 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Compute ROUGE -L Scores Before Fine Tuning\n","metadata":{}},{"cell_type":"code","source":"from rouge_score import rouge_scorer\nimport numpy as np\n\ndef compute_rouge_l(dataset, model, tokenizer, num_samples=10):\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    scores = []\n    model.eval()\n    \n    for example in dataset.select(range(min(num_samples, len(dataset)))):\n        question = example[\"Question\"]\n        ground_truth = example[\"Response\"]\n        \n        convo = [{\"role\": \"user\", \"content\": question}]\n        inputs = tokenizer(\n            [tokenizer.apply_chat_template(convo, tokenize=False)],\n            return_tensors=\"pt\"\n        ).to(\"cuda\")\n        \n        outputs = model.generate(**inputs, max_new_tokens=1200, use_cache=True)\n        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        score = scorer.score(ground_truth, prediction)['rougeL'].fmeasure\n        scores.append(score)\n    \n    return np.mean(scores)\n\npre_finetune_rouge= compute_rouge_l(dataset_dict[\"validation\"],model_lora,tokenizer)\nrun =wandb.init(\n    project=\"Parameter-Efficient Supervised Fine-Tuning of LLaM\",\n    job_type=\"training\",\n    anonymous=\"allow\",\n    \n)\nwandb.log({\"pre_finetune_rouge_l\":pre_finetune_rouge})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:06:41.764843Z","iopub.execute_input":"2025-06-26T03:06:41.765067Z","iopub.status.idle":"2025-06-26T03:09:15.697328Z","shell.execute_reply.started":"2025-06-26T03:06:41.765051Z","shell.execute_reply":"2025-06-26T03:09:15.696585Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250626_030909-75ddp9tv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hamayoonali38-datacamp/Parameter-Efficient%20Supervised%20Fine-Tuning%20of%20LLaM/runs/75ddp9tv?apiKey=41d2ec437ff04feebd16706d555e08e5c39e619d' target=\"_blank\">avid-universe-5</a></strong> to <a href='https://wandb.ai/hamayoonali38-datacamp/Parameter-Efficient%20Supervised%20Fine-Tuning%20of%20LLaM?apiKey=41d2ec437ff04feebd16706d555e08e5c39e619d' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hamayoonali38-datacamp/Parameter-Efficient%20Supervised%20Fine-Tuning%20of%20LLaM?apiKey=41d2ec437ff04feebd16706d555e08e5c39e619d' target=\"_blank\">https://wandb.ai/hamayoonali38-datacamp/Parameter-Efficient%20Supervised%20Fine-Tuning%20of%20LLaM?apiKey=41d2ec437ff04feebd16706d555e08e5c39e619d</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hamayoonali38-datacamp/Parameter-Efficient%20Supervised%20Fine-Tuning%20of%20LLaM/runs/75ddp9tv?apiKey=41d2ec437ff04feebd16706d555e08e5c39e619d' target=\"_blank\">https://wandb.ai/hamayoonali38-datacamp/Parameter-Efficient%20Supervised%20Fine-Tuning%20of%20LLaM/runs/75ddp9tv?apiKey=41d2ec437ff04feebd16706d555e08e5c39e619d</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## intialize Fine-Tuning Trainer","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model=model_lora,\n    tokenizer=tokenizer,\n    train_dataset=dataset_dict['train'],\n    eval_dataset=dataset_dict['validation'],\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n\n    args=TrainingArguments(\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,  \n        warmup_steps=100,\n        max_steps=250,\n        learning_rate=2e-4,  \n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",  \n        seed=3407,\n        output_dir=\"outputs\",\n\n        report_to=\"wandb\",  \n        run_name=\"llama3-3B-lr2e-4-steps1000\",\n        save_strategy='steps',\n        save_steps=50,\n        save_total_limit=1,\n        eval_strategy=\"steps\",  # ✅ changed from evaluation_strategy\n        eval_steps=50,\n\n        hub_model_id=\"Hamayyoon/LLaMA3.2B-Medcot\",\n        logging_first_step=True,  \n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:09:15.698205Z","iopub.execute_input":"2025-06-26T03:09:15.698476Z","iopub.status.idle":"2025-06-26T03:09:17.125289Z","shell.execute_reply.started":"2025-06-26T03:09:15.698454Z","shell.execute_reply":"2025-06-26T03:09:17.124559Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"421c2c73bf714995834600f8e88b71b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d26b3ab5ca3d41d88c64f547fcaebb21"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## model Training","metadata":{}},{"cell_type":"code","source":"trainer_state = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T03:09:17.126265Z","iopub.execute_input":"2025-06-26T03:09:17.126581Z","iopub.status.idle":"2025-06-26T05:28:40.441415Z","shell.execute_reply.started":"2025-06-26T03:09:17.126532Z","shell.execute_reply":"2025-06-26T05:28:40.440672Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 400 | Num Epochs = 21 | Total steps = 250\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n \"-____-\"     Trainable parameters = 24,313,856/3,000,000,000 (0.81% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 2:18:42, Epoch 19/21]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.554300</td>\n      <td>1.541682</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.343100</td>\n      <td>1.453986</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.008900</td>\n      <td>1.633494</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.668900</td>\n      <td>2.065748</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.461100</td>\n      <td>2.326826</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"post_finetune_rouge = compute_rouge_l(dataset_dict[\"validation\"], model_lora, tokenizer)\nwandb.log({\"post_finetune_rouge_l\": post_finetune_rouge})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:19:52.376681Z","iopub.execute_input":"2025-06-26T06:19:52.377289Z","iopub.status.idle":"2025-06-26T06:26:14.569506Z","shell.execute_reply.started":"2025-06-26T06:19:52.377266Z","shell.execute_reply":"2025-06-26T06:26:14.568929Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model_lora.save_pretrained(\"lora_adapters\")\ntokenizer.save_pretrained(\"lora_adapters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:27:32.713713Z","iopub.execute_input":"2025-06-26T06:27:32.714322Z","iopub.status.idle":"2025-06-26T06:27:33.560852Z","shell.execute_reply.started":"2025-06-26T06:27:32.714297Z","shell.execute_reply":"2025-06-26T06:27:33.560096Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('lora_adapters/tokenizer_config.json',\n 'lora_adapters/special_tokens_map.json',\n 'lora_adapters/tokenizer.json')"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Load custom token\nuser_secrets = UserSecretsClient()\nupload_token = user_secrets.get_secret(\"hf_token\")\n\n# Log in with that token\nlogin(token=upload_token)\n\nmodel_lora.push_to_hub(\"Hamayyoon/medcot-llama3.2-3b-model\", token=upload_token)\ntokenizer.push_to_hub(\"Hamayyoon/medcot-llama3.2-3b-tokenizer\", token=upload_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:29:34.348212Z","iopub.execute_input":"2025-06-26T06:29:34.348979Z","iopub.status.idle":"2025-06-26T06:29:39.483931Z","shell.execute_reply.started":"2025-06-26T06:29:34.348953Z","shell.execute_reply":"2025-06-26T06:29:39.483343Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/97.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1f78668e07743dcb61032f3a8d3258e"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"Saved model to https://huggingface.co/Hamayyoon/medcot-llama3.2-3b-model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d0e9befa4b04159b8c3126cccd06097"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d016e76ebcf45cd8cc6d44bfaa7a6f5"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def generate_response(question, model, tokenizer):\n    \n    FastLanguageModel.for_inference(model)\n    convo = [{\"role\": \"user\", \"content\": question}]\n    inputs = tokenizer([tokenizer.apply_chat_template(convo, tokenize=False)], return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=1200, use_cache=True)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response.split(\"<response>\")[1].split(\"</response>\")[0] if \"<response>\" in response else response\n\nquestion = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no \nleakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, \nwhat would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n\nresponse = generate_response(question, model_lora, tokenizer)\nprint(f\"Response: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:30:20.647912Z","iopub.execute_input":"2025-06-26T06:30:20.648200Z","iopub.status.idle":"2025-06-26T06:30:56.355797Z","shell.execute_reply.started":"2025-06-26T06:30:20.648181Z","shell.execute_reply":"2025-06-26T06:30:56.355009Z"}},"outputs":[{"name":"stdout","text":"Response: In this scenario, the symptoms and the Q-tip test suggest stress urinary incontinence, which is involuntary urine leakage accompanied by physical stress or exertion. \n\nOn cystometry, you would typically see a normal residual volume (RV) because there's no indication of neurological bladder dysfunction that would affect emptying. \n\nRegarding detrusor contractions, under stress or exertional pressure, such as during activities like coughing or sneezing, you would likely observe detrusor contractions. These contractions are a normal response of the bladder muscle to increased pressure, leading to the leakage of urine. However, it's important to note that the frequency and amplitude of these contractions might not be constant and could vary with different levels of exertion. \n\nOverall, the findings on cystometry would align with stress urinary incontinence, confirming the suspicion based on her symptoms and the Q-tip test.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:32:08.461217Z","iopub.execute_input":"2025-06-26T06:32:08.461714Z","iopub.status.idle":"2025-06-26T06:32:09.079872Z","shell.execute_reply.started":"2025-06-26T06:32:08.461691Z","shell.execute_reply":"2025-06-26T06:32:09.079290Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▂▁▂▆█</td></tr><tr><td>eval/runtime</td><td>█▂▂▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▇█▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▇▇█▇</td></tr><tr><td>post_finetune_rouge_l</td><td>█▁</td></tr><tr><td>pre_finetune_rouge_l</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▄▄▃▂▂▁▁▁▁▂▂▃▃▅▄▅▆▆▇▇█▇▇▇▇▇</td></tr><tr><td>train/learning_rate</td><td>▁▂▂▃▄▄▅▆▇▇██▇▇▆▆▅▅▄▄▃▃▂▂▂▁</td></tr><tr><td>train/loss</td><td>██▇▇▆▅▅▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.32683</td></tr><tr><td>eval/runtime</td><td>47.18</td></tr><tr><td>eval/samples_per_second</td><td>2.12</td></tr><tr><td>eval/steps_per_second</td><td>0.276</td></tr><tr><td>post_finetune_rouge_l</td><td>0.12072</td></tr><tr><td>pre_finetune_rouge_l</td><td>0.20637</td></tr><tr><td>total_flos</td><td>1.1233494505301606e+17</td></tr><tr><td>train/epoch</td><td>19.24</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/grad_norm</td><td>0.73752</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4611</td></tr><tr><td>train_loss</td><td>1.15136</td></tr><tr><td>train_runtime</td><td>8361.1128</td></tr><tr><td>train_samples_per_second</td><td>0.957</td></tr><tr><td>train_steps_per_second</td><td>0.03</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">avid-universe-5</strong> at: <a href='https://wandb.ai/hamayoonali38-datacamp/Parameter-Efficient%20Supervised%20Fine-Tuning%20of%20LLaM/runs/75ddp9tv?apiKey=41d2ec437ff04feebd16706d555e08e5c39e619d' target=\"_blank\">https://wandb.ai/hamayoonali38-datacamp/Parameter-Efficient%20Supervised%20Fine-Tuning%20of%20LLaM/runs/75ddp9tv?apiKey=41d2ec437ff04feebd16706d555e08e5c39e619d</a><br> View project at: <a href='https://wandb.ai/hamayoonali38-datacamp/Parameter-Efficient%20Supervised%20Fine-Tuning%20of%20LLaM?apiKey=41d2ec437ff04feebd16706d555e08e5c39e619d' target=\"_blank\">https://wandb.ai/hamayoonali38-datacamp/Parameter-Efficient%20Supervised%20Fine-Tuning%20of%20LLaM?apiKey=41d2ec437ff04feebd16706d555e08e5c39e619d</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250626_030909-75ddp9tv/logs</code>"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"##   ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}